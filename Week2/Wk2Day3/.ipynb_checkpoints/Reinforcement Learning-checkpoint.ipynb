{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 1.9.6\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\Program Files\\Python37\\Lib\\site-packages')\n",
    "\n",
    "import pygame\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 5, 3)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        width = (960//2//2//2//2 - 2)//2\n",
    "        height = (540//2//2//2//2 - 2)//2\n",
    "        self.fc1 = nn.Linear(width*height*5, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        self.fc3 = nn.Linear(10, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilonGreedy(Qnet, game, epsilon):\n",
    "    moves = ['up', 'down', 'stay']\n",
    "    random.shuffle(moves)\n",
    "    # random move\n",
    "    if np.random.uniform() < epsilon:\n",
    "        move = moves[random.sample(range(len(moves)), 1)[0]]\n",
    "        game.makeMove(move, 'left')\n",
    "        Q = Qnet(game.draw())\n",
    "        # Undo the move\n",
    "        if move == 'up':\n",
    "            game.makeMove('down', 'left')\n",
    "        elif move == 'down':\n",
    "            game.makeMove('up', 'left')\n",
    "    # greedy move\n",
    "    else:\n",
    "        qs = []\n",
    "        for m in moves:\n",
    "            game.makeMove(m, 'left')\n",
    "            qs.append(Qnet(game.draw()))\n",
    "            # Undo the move\n",
    "            if m == 'up':\n",
    "                game.makeMove('down', 'left')\n",
    "            elif m == 'down':\n",
    "                game.makeMove('up', 'left')\n",
    "        move = moves[np.argmax(qs)]\n",
    "        Q = np.max(qs)\n",
    "    return move, Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomMover(state):\n",
    "    return random.choice(['up', 'down', 'stay'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyMover(game):\n",
    "    return game.bestMove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainQnet(Qnet, optim, lossFunc, nReps, nIterations, epsilon, epsilonDecayFactor, game, opponentStrategyF):\n",
    "    outcomes = np.zeros(nReps)\n",
    "    quickWinRatio = np.zeros(nReps)\n",
    "\n",
    "    for rep in range(nReps):\n",
    "        if rep > 0:\n",
    "            epsilon *= epsilonDecayFactor\n",
    "        step = 0\n",
    "        done = False\n",
    "        \n",
    "        states = torch.tensor([])\n",
    "        Qs = torch.tensor([])\n",
    "        \n",
    "        # Don't let the game bug out\n",
    "        game.clock.tick()\n",
    "        \n",
    "        # Pick our initial move\n",
    "        move, Qnext = epsilonGreedy(Qnet, game, epsilon)\n",
    "        while not done:\n",
    "            \n",
    "            # Neural network makes move\n",
    "            game.makeMove(move, 'left')\n",
    "            \n",
    "            # Opponent makes move\n",
    "            if not done:\n",
    "                muv = opponentStrategyF(game)\n",
    "                game.makeMove(muv, 'right')\n",
    "                \n",
    "            # Game updates ball position and checks if victory occurred\n",
    "            done, Qgame = game.update()\n",
    "            \n",
    "            # Game returns its pixel values\n",
    "            image = game.draw()\n",
    "            \n",
    "            # Game draws to screen\n",
    "            game.display()\n",
    "                \n",
    "            if not done:\n",
    "                # Figure out how we should next move and what the new score is\n",
    "                move, Qnext = epsilonGreedy(Qnet, game, epsilon)\n",
    "                Qgame = torch.clamp(Qnext + Qgame, -1, 1)\n",
    "            else:\n",
    "                # Record and print this outcome\n",
    "                outcomes[rep] = Qgame\n",
    "                print('Repetition:', rep, \"Epsilon:\", epsilon)\n",
    "                print(\"Win ratio:\", sum(outcomes == 1)/(rep+1))\n",
    "                back = rep - 49 if rep > 49 else 0\n",
    "                bot = 50 if rep > 49 else rep + 1\n",
    "                quickWinRatio[rep] = sum(outcomes[back:rep+1] == 1)/bot\n",
    "                print(\"Win ratio past 50:\", quickWinRatio[rep])\n",
    "               \n",
    "            # Store the states and Qs so we can train at end\n",
    "            states = torch.cat((states, image), dim=0)\n",
    "            Qs = torch.cat((Qs, Qgame), dim=0)\n",
    "            \n",
    "        for i in range(nIterations):\n",
    "            optim.zero_grad()\n",
    "            out = network(states)\n",
    "            loss = lossFunc(out, Qs)\n",
    "            loss.backward(retain_graph=True)\n",
    "            optimizer.step()\n",
    "\n",
    "    print('DONE')\n",
    "    return Qnet, outcomes, quickWinRatio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition: 0 Epsilon: 0.3\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 1 Epsilon: 0.29969999999999997\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 2 Epsilon: 0.29940029999999995\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 3 Epsilon: 0.2991008997\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 4 Epsilon: 0.2988017988003\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 5 Epsilon: 0.2985029970014997\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 6 Epsilon: 0.29820449400449817\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 7 Epsilon: 0.2979062895104937\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 8 Epsilon: 0.2976083832209832\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 9 Epsilon: 0.29731077483776225\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 10 Epsilon: 0.2970134640629245\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 11 Epsilon: 0.2967164505988616\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 12 Epsilon: 0.2964197341482627\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 13 Epsilon: 0.29612331441411444\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 14 Epsilon: 0.29582719109970035\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 15 Epsilon: 0.29553136390860063\n",
      "Win ratio: 0.0\n",
      "Win ratio past 50: 0.0\n",
      "Repetition: 16 Epsilon: 0.29523583254469205\n",
      "Win ratio: 0.058823529411764705\n",
      "Win ratio past 50: 0.058823529411764705\n",
      "Repetition: 17 Epsilon: 0.29494059671214734\n",
      "Win ratio: 0.05555555555555555\n",
      "Win ratio past 50: 0.05555555555555555\n",
      "Repetition: 18 Epsilon: 0.2946456561154352\n",
      "Win ratio: 0.05263157894736842\n",
      "Win ratio past 50: 0.05263157894736842\n",
      "Repetition: 19 Epsilon: 0.2943510104593198\n",
      "Win ratio: 0.05\n",
      "Win ratio past 50: 0.05\n",
      "Repetition: 20 Epsilon: 0.29405665944886045\n",
      "Win ratio: 0.047619047619047616\n",
      "Win ratio past 50: 0.047619047619047616\n",
      "Repetition: 21 Epsilon: 0.2937626027894116\n",
      "Win ratio: 0.09090909090909091\n",
      "Win ratio past 50: 0.09090909090909091\n",
      "Repetition: 22 Epsilon: 0.29346884018662217\n",
      "Win ratio: 0.08695652173913043\n",
      "Win ratio past 50: 0.08695652173913043\n",
      "Repetition: 23 Epsilon: 0.29317537134643556\n",
      "Win ratio: 0.08333333333333333\n",
      "Win ratio past 50: 0.08333333333333333\n",
      "Repetition: 24 Epsilon: 0.29288219597508913\n",
      "Win ratio: 0.12\n",
      "Win ratio past 50: 0.12\n",
      "Repetition: 25 Epsilon: 0.29258931377911407\n",
      "Win ratio: 0.11538461538461539\n",
      "Win ratio past 50: 0.11538461538461539\n",
      "Repetition: 26 Epsilon: 0.29229672446533495\n",
      "Win ratio: 0.1111111111111111\n",
      "Win ratio past 50: 0.1111111111111111\n",
      "Repetition: 27 Epsilon: 0.2920044277408696\n",
      "Win ratio: 0.10714285714285714\n",
      "Win ratio past 50: 0.10714285714285714\n",
      "Repetition: 28 Epsilon: 0.29171242331312874\n",
      "Win ratio: 0.10344827586206896\n",
      "Win ratio past 50: 0.10344827586206896\n",
      "Repetition: 29 Epsilon: 0.29142071088981564\n",
      "Win ratio: 0.1\n",
      "Win ratio past 50: 0.1\n",
      "Repetition: 30 Epsilon: 0.2911292901789258\n",
      "Win ratio: 0.0967741935483871\n",
      "Win ratio past 50: 0.0967741935483871\n",
      "Repetition: 31 Epsilon: 0.29083816088874687\n",
      "Win ratio: 0.09375\n",
      "Win ratio past 50: 0.09375\n",
      "Repetition: 32 Epsilon: 0.2905473227278581\n",
      "Win ratio: 0.09090909090909091\n",
      "Win ratio past 50: 0.09090909090909091\n",
      "Repetition: 33 Epsilon: 0.29025677540513023\n",
      "Win ratio: 0.08823529411764706\n",
      "Win ratio past 50: 0.08823529411764706\n",
      "Repetition: 34 Epsilon: 0.2899665186297251\n",
      "Win ratio: 0.08571428571428572\n",
      "Win ratio past 50: 0.08571428571428572\n",
      "Repetition: 35 Epsilon: 0.2896765521110954\n",
      "Win ratio: 0.08333333333333333\n",
      "Win ratio past 50: 0.08333333333333333\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b8771abb5b6e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mpongGame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpong\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPong\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mQnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutcome\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msamples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mquickWinRatio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrainQnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnReps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnIterations\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilonDecayFactor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpongGame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstrat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-fffc70286fea>\u001b[0m in \u001b[0;36mtrainQnet\u001b[1;34m(Qnet, optim, lossFunc, nReps, nIterations, epsilon, epsilonDecayFactor, game, opponentStrategyF)\u001b[0m\n\u001b[0;32m     38\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m                 \u001b[1;31m# Figure out how we should next move and what the new score is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m                 \u001b[0mmove\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mQnext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mepsilonGreedy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQnet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     41\u001b[0m                 \u001b[0mQgame\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclamp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQnext\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mQgame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-9da81451493a>\u001b[0m in \u001b[0;36mepsilonGreedy\u001b[1;34m(Qnet, game, epsilon)\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmoves\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m             \u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakeMove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m             \u001b[0mqs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mQnet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m             \u001b[1;31m# Undo the move\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'up'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Day8\\pong.py\u001b[0m in \u001b[0;36mdraw\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    256\u001b[0m                         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mblit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcountdown\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_width\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcountdown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_width\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_height\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mcountdown\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_height\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixels_red\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixels_blue\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mpygame\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msurfarray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpixels_green\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscreen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m                 \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMaxPool2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m                 \u001b[0mimage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#network = CNN()#torch.load('pongSave.pt')\n",
    "network = torch.load('pongSave.pt')\n",
    "optimizer = optim.SGD(network.parameters(), lr=0.0005)\n",
    "loss = nn.MSELoss()\n",
    "\n",
    "nReps = 100000\n",
    "nIterations = 1\n",
    "epsilon = 0.3\n",
    "epsilonDecayFactor = 0.999 \n",
    "\n",
    "strat = policyMover\n",
    "\n",
    "pygame.init()\n",
    "pongGame = pong.Pong()\n",
    "\n",
    "Qnet, outcome, steps, samples, quickWinRatio = trainQnet(network, optimizer, loss, nReps, nIterations, epsilon, epsilonDecayFactor, pongGame, strat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(network, 'pongSave.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CNN' object has no attribute 'width'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-122955301dde>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Program Files\\Python37\\Lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    537\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    538\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m--> 539\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m    540\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    541\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CNN' object has no attribute 'width'"
     ]
    }
   ],
   "source": [
    "print(network.width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
